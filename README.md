# XOR_Classification-using-ML
This project demonstrates why neural networks require hidden layers by solving the classic XOR classification problem, which is not linearly separable.

The work starts with a visualization of the XOR data in 2D space, showing that a single linear decision boundary cannot separate the classes. To overcome this, the project introduces hidden neurons that transform the input space using logical operations (OR, AND), making the problem linearly separable in a transformed feature space.

**The project further explores:**

-- The role of non-linear activation functions

-- The effect of weights and bias

-- Loss comparison between linear output and sigmoid output

-- Intuition behind backpropagation

-- Visualization of how feature transformation enables correct classification

This project focuses on conceptual clarity and visualization, rather than heavy model complexity.

**Objectives**

-- Understand why XOR cannot be solved using a single linear model

-- Learn the necessity of hidden layers in neural networks

-- Visualize how hidden layers transform the input space

-- Compare linear vs non-linear outputs using loss functions

-- Build strong intuition for future ML problems
